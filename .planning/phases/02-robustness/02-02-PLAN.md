---
phase: 02-robustness
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/actions/process-video.ts
  - src/components/processing-status.tsx
  - src/components/transcript-view.tsx
  - src/components/results-tabs.tsx
  - src/app/page.tsx
  - next.config.ts
autonomous: true

must_haves:
  truths:
    - "When YouTube captions unavailable, app automatically transcribes via AssemblyAI"
    - "User sees informative status during processing (acknowledging it may take longer)"
    - "Transcript view shows source indicator (YouTube captions vs Audio transcription)"
    - "Transcript shows speaker labels when AssemblyAI detects multiple speakers"
    - "Long transcriptions complete without timeout errors"
  artifacts:
    - path: "src/actions/process-video.ts"
      provides: "Fallback transcription logic"
      contains: "transcribeAudio"
    - path: "src/components/transcript-view.tsx"
      provides: "Source indicator and speaker label display"
      contains: "TranscriptSource"
    - path: "src/components/processing-status.tsx"
      provides: "Updated status messages for processing"
      contains: "fetching-transcript"
    - path: "src/components/results-tabs.tsx"
      provides: "Prop threading to TranscriptView"
      contains: "transcriptSource"
  key_links:
    - from: "src/actions/process-video.ts"
      to: "src/lib/assemblyai/transcribe.ts"
      via: "imports transcribeAudio for fallback"
      pattern: "import.*transcribeAudio"
    - from: "src/app/page.tsx"
      to: "src/components/results-tabs.tsx"
      via: "passes transcriptSource and hasSpeakers props"
      pattern: "transcriptSource"
    - from: "src/components/results-tabs.tsx"
      to: "src/components/transcript-view.tsx"
      via: "passes source and hasSpeakers to TranscriptView"
      pattern: "source=.*transcriptSource"
---

<objective>
Integrate AssemblyAI fallback into video processing pipeline and update UI to show transcript source and speaker labels.

Purpose: Complete the robustness feature by wiring backend to frontend and ensuring users understand when audio transcription is used.
Output: Working end-to-end flow where videos without YouTube captions are automatically transcribed via AssemblyAI with proper UI feedback.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-robustness/02-RESEARCH.md
@.planning/phases/02-robustness/02-01-SUMMARY.md

# Files to modify
@src/actions/process-video.ts
@src/components/processing-status.tsx
@src/components/transcript-view.tsx
@src/components/results-tabs.tsx
@src/app/page.tsx
@src/types/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update process-video.ts with AssemblyAI fallback</name>
  <files>
    - src/actions/process-video.ts
  </files>
  <action>
Update `src/actions/process-video.ts` to:
1. Try YouTube captions first (existing behavior)
2. Fall back to AssemblyAI transcription on failure
3. Return transcript source information to frontend

```typescript
'use server';

import { extractVideoId, fetchVideoMetadata, fetchTranscript, formatTranscriptIntoParagraphs } from '@/lib/youtube';
import { transcribeAudio } from '@/lib/assemblyai/transcribe';
import { VideoMetadata, TranscriptSegment, TranscriptSource, SpeakerUtterance } from '@/types';

// Allow up to 5 minutes for AssemblyAI transcription
// This is required for Vercel deployment
// Note: Vercel Hobby plan max is 60s, Pro plan allows up to 300s
export const maxDuration = 300;

export interface ProcessVideoResult {
  success: true;
  videoId: string;
  metadata: VideoMetadata;
  transcript: string;
  rawSegments: TranscriptSegment[];
  transcriptSource: TranscriptSource;
  utterances?: SpeakerUtterance[];
  hasSpeakers: boolean;
}

export interface ProcessVideoError {
  success: false;
  error: string;
  metadata?: VideoMetadata;
}

export async function processVideo(url: string): Promise<ProcessVideoResult | ProcessVideoError> {
  // Step 1: Extract and validate video ID
  const videoId = extractVideoId(url);
  if (!videoId) {
    return { success: false, error: 'Invalid YouTube URL. Please check the URL and try again.' };
  }

  // Step 2: Fetch metadata
  let metadata: VideoMetadata;
  try {
    metadata = await fetchVideoMetadata(videoId);
  } catch (error) {
    return {
      success: false,
      error: 'Could not find video. Please check the URL and try again.',
    };
  }

  // Step 3: Try YouTube captions first
  let rawSegments: TranscriptSegment[];
  let transcript: string;
  let transcriptSource: TranscriptSource = 'youtube';
  let utterances: SpeakerUtterance[] | undefined;
  let hasSpeakers = false;

  try {
    rawSegments = await fetchTranscript(videoId);
    transcript = formatTranscriptIntoParagraphs(rawSegments);
  } catch (youtubeError) {
    // Step 4: Fall back to AssemblyAI
    // Note: This can take 30-130+ seconds for audio extraction + transcription
    try {
      const result = await transcribeAudio(videoId);
      transcript = result.text;
      rawSegments = result.segments;
      transcriptSource = 'assemblyai';
      utterances = result.utterances;
      hasSpeakers = result.hasSpeakers;
    } catch (assemblyError) {
      // Both methods failed - return original YouTube error for clarity
      return {
        success: false,
        error: youtubeError instanceof Error
          ? youtubeError.message
          : 'Failed to fetch transcript. This video may not have captions and audio transcription failed.',
        metadata,
      };
    }
  }

  return {
    success: true,
    videoId,
    metadata,
    transcript,
    rawSegments,
    transcriptSource,
    utterances,
    hasSpeakers,
  };
}
```

Key implementation notes:
- Preserve original YouTube error message when both methods fail
- Track source for UI display
- Pass through utterances and hasSpeakers for speaker label display
- maxDuration export moved here (removed from Task 3)
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep "transcribeAudio" src/actions/process-video.ts` shows import
    - `grep "transcriptSource" src/actions/process-video.ts` shows return field
    - `grep "maxDuration" src/actions/process-video.ts` shows export
  </verify>
  <done>
    - YouTube captions tried first (preserves free/fast path)
    - AssemblyAI fallback activates on YouTube failure
    - Result includes transcriptSource, utterances, hasSpeakers
    - maxDuration configured for long-running transcriptions
  </done>
</task>

<task type="auto">
  <name>Task 2a: Update leaf components (ProcessingStatus, TranscriptView)</name>
  <files>
    - src/components/processing-status.tsx
    - src/components/transcript-view.tsx
  </files>
  <action>
1. Update `src/components/processing-status.tsx` with improved messaging.

Note on UX limitation: Server actions cannot stream status updates mid-execution. The user will see 'fetching-transcript' status for the entire duration (which can be 30-130 seconds if AssemblyAI fallback is triggered). We improve UX by making the message acknowledge this possibility upfront.

Replace the entire file:
```typescript
'use client';

import { Loader2 } from 'lucide-react';
import { ProcessingStatus as Status } from '@/types';

interface ProcessingStatusProps {
  status: Status;
}

const statusMessages: Record<Status, string> = {
  idle: '',
  'fetching-metadata': 'Fetching video information...',
  'fetching-transcript': 'Retrieving transcript (may take up to 2 minutes if audio transcription is needed)...',
  summarizing: 'Generating summary...',
  complete: 'Done!',
  error: '',
};

export function ProcessingStatus({ status }: ProcessingStatusProps) {
  if (status === 'idle' || status === 'error') {
    return null;
  }

  const isLoading = status !== 'complete';
  const message = statusMessages[status];

  return (
    <div className="flex items-center gap-2 text-sm text-muted-foreground">
      {isLoading && (
        <Loader2 className="h-4 w-4 animate-spin" />
      )}
      <span>{message}</span>
    </div>
  );
}
```

2. Update `src/components/transcript-view.tsx` to show source and speaker labels.

Replace the entire file:
```typescript
'use client';

import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { CopyButton } from './copy-button';
import { TranscriptSource } from '@/types';

interface TranscriptViewProps {
  transcript: string;
  source?: TranscriptSource;
  hasSpeakers?: boolean;
}

export function TranscriptView({ transcript, source = 'youtube', hasSpeakers = false }: TranscriptViewProps) {
  const sourceLabel = source === 'youtube'
    ? 'YouTube captions'
    : hasSpeakers
      ? 'Audio transcription (with speakers)'
      : 'Audio transcription';

  return (
    <Card>
      <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
        <div className="space-y-1">
          <CardTitle className="text-lg font-medium">Full Transcript</CardTitle>
          <p className="text-xs text-muted-foreground">
            Source: {sourceLabel}
          </p>
        </div>
        <CopyButton text={transcript} />
      </CardHeader>
      <CardContent>
        <div className="prose prose-sm max-w-none dark:prose-invert">
          <div
            className="whitespace-pre-wrap text-sm leading-relaxed"
            dangerouslySetInnerHTML={{
              __html: formatTranscriptHtml(transcript, hasSpeakers)
            }}
          />
        </div>
      </CardContent>
    </Card>
  );
}

/**
 * Convert markdown speaker labels to HTML for display.
 * Only processes **Speaker X:** patterns when hasSpeakers is true.
 */
function formatTranscriptHtml(text: string, hasSpeakers: boolean): string {
  if (!hasSpeakers) {
    // No speakers - just escape HTML and preserve line breaks
    return escapeHtml(text);
  }

  // Convert **Speaker X:** to styled spans
  return escapeHtml(text).replace(
    /\*\*Speaker ([A-Z]):\*\*/g,
    '<strong class="text-primary">Speaker $1:</strong>'
  );
}

function escapeHtml(text: string): string {
  return text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&#039;');
}
```

Note: Props are optional with defaults for backward compatibility.
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep "may take up to 2 minutes" src/components/processing-status.tsx` shows improved message
    - `grep "TranscriptSource" src/components/transcript-view.tsx` shows import
    - `grep "source?" src/components/transcript-view.tsx` shows optional prop
  </verify>
  <done>
    - ProcessingStatus shows message that acknowledges potential wait time upfront
    - TranscriptView accepts source and hasSpeakers props (optional for backward compat)
    - TranscriptView shows source label (YouTube captions vs Audio transcription)
    - Speaker labels rendered with styling when hasSpeakers is true
  </done>
</task>

<task type="auto">
  <name>Task 2b: Update prop threading (ResultsTabs, page.tsx)</name>
  <files>
    - src/components/results-tabs.tsx
    - src/app/page.tsx
  </files>
  <action>
1. Update `src/components/results-tabs.tsx` to accept and pass through transcript source props.

Replace the entire file:
```typescript
'use client';

import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { SummaryView } from './summary-view';
import { TranscriptView } from './transcript-view';
import { TranscriptSource } from '@/types';

interface ResultsTabsProps {
  summary: string;
  transcript: string;
  transcriptSource?: TranscriptSource;
  hasSpeakers?: boolean;
  isSummarizing?: boolean;
}

export function ResultsTabs({
  summary,
  transcript,
  transcriptSource = 'youtube',
  hasSpeakers = false,
  isSummarizing
}: ResultsTabsProps) {
  return (
    <Tabs defaultValue="summary" className="w-full">
      <TabsList className="grid w-full grid-cols-2">
        <TabsTrigger value="summary">Summary</TabsTrigger>
        <TabsTrigger value="transcript">Transcript</TabsTrigger>
      </TabsList>
      <TabsContent value="summary" className="mt-4">
        <SummaryView summary={summary} isLoading={isSummarizing && !summary} />
      </TabsContent>
      <TabsContent value="transcript" className="mt-4">
        <TranscriptView
          transcript={transcript}
          source={transcriptSource}
          hasSpeakers={hasSpeakers}
        />
      </TabsContent>
    </Tabs>
  );
}
```

2. Update `src/app/page.tsx` to track and pass transcript source state.

Replace the entire file:
```typescript
'use client';

import { useState, useCallback } from 'react';
import { UrlInput } from '@/components/url-input';
import { VideoPreview } from '@/components/video-preview';
import { ProcessingStatus } from '@/components/processing-status';
import { ResultsTabs } from '@/components/results-tabs';
import { processVideo } from '@/actions/process-video';
import { VideoMetadata, ProcessingStatus as Status, TranscriptSource } from '@/types';

export default function Home() {
  const [status, setStatus] = useState<Status>('idle');
  const [error, setError] = useState<string | null>(null);
  const [metadata, setMetadata] = useState<VideoMetadata | null>(null);
  const [transcript, setTranscript] = useState<string>('');
  const [summary, setSummary] = useState<string>('');
  const [transcriptSource, setTranscriptSource] = useState<TranscriptSource>('youtube');
  const [hasSpeakers, setHasSpeakers] = useState(false);

  const handleSubmit = useCallback(async (videoId: string, url: string) => {
    // Reset state
    setError(null);
    setMetadata(null);
    setTranscript('');
    setSummary('');
    setTranscriptSource('youtube');
    setHasSpeakers(false);

    // Step 1: Process video (metadata + transcript)
    // Note: This shows 'fetching-transcript' which may take 30-130s if AssemblyAI fallback is used
    setStatus('fetching-metadata');

    const result = await processVideo(url);

    if (!result.success) {
      setError(result.error);
      if (result.metadata) {
        setMetadata(result.metadata);
      }
      setStatus('error');
      return;
    }

    setMetadata(result.metadata);
    setTranscript(result.transcript);
    setTranscriptSource(result.transcriptSource);
    setHasSpeakers(result.hasSpeakers);
    setStatus('summarizing');

    // Step 2: Stream summarization
    try {
      const response = await fetch('/api/summarize', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          transcript: result.transcript,
          videoTitle: result.metadata.title,
        }),
      });

      if (!response.ok) {
        throw new Error('Failed to generate summary');
      }

      // Handle streaming response
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      if (!reader) {
        throw new Error('No response body');
      }

      let accumulatedSummary = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        accumulatedSummary += chunk;
        setSummary(accumulatedSummary);
      }

      setStatus('complete');
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Failed to generate summary');
      setStatus('error');
    }
  }, []);

  return (
    <main className="min-h-screen bg-background">
      <div className="container max-w-3xl mx-auto py-8 px-4 space-y-6">
        <div className="text-center space-y-2">
          <h1 className="text-3xl font-bold">YouTube Summarizer</h1>
          <p className="text-muted-foreground">
            Paste a YouTube URL to get a transcript and AI-generated summary
          </p>
        </div>

        <UrlInput
          onSubmit={handleSubmit}
          isLoading={status !== 'idle' && status !== 'complete' && status !== 'error'}
        />

        <ProcessingStatus status={status} />

        {error && (
          <div className="bg-destructive/10 text-destructive px-4 py-3 rounded-md text-sm">
            {error}
          </div>
        )}

        {metadata && (
          <VideoPreview
            title={metadata.title}
            authorName={metadata.authorName}
            thumbnailUrl={metadata.thumbnailUrl}
            loading={status === 'fetching-metadata'}
          />
        )}

        {(transcript || summary) && (
          <ResultsTabs
            summary={summary}
            transcript={transcript}
            transcriptSource={transcriptSource}
            hasSpeakers={hasSpeakers}
            isSummarizing={status === 'summarizing'}
          />
        )}
      </div>
    </main>
  );
}
```
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep "transcriptSource" src/components/results-tabs.tsx` shows prop
    - `grep "TranscriptSource" src/app/page.tsx` shows import
    - `grep "setTranscriptSource" src/app/page.tsx` shows state setter
    - `grep "transcriptSource={transcriptSource}" src/app/page.tsx` shows prop passing
  </verify>
  <done>
    - ResultsTabs accepts transcriptSource and hasSpeakers props
    - ResultsTabs passes props through to TranscriptView
    - page.tsx tracks transcriptSource and hasSpeakers state
    - page.tsx passes props to ResultsTabs
    - All props flow from page.tsx -> ResultsTabs -> TranscriptView
  </done>
</task>

<task type="auto">
  <name>Task 3: Configure Next.js for long-running actions</name>
  <files>
    - next.config.ts
  </files>
  <action>
AssemblyAI transcription can take 30-130+ seconds. Configure Next.js to handle this.

Check if `next.config.ts` exists, if not create it. If `next.config.js` or `next.config.mjs` exists, use that instead.

Add or update the config:

```typescript
// next.config.ts (or update existing)
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  experimental: {
    serverActions: {
      bodySizeLimit: '10mb',  // Audio buffers can be large
    },
  },
};

export default nextConfig;
```

Note: The maxDuration export is in process-video.ts (Task 1), not here.
That's the Vercel-specific function timeout configuration.
  </action>
  <verify>
    - `grep "serverActions" next.config.ts` (or next.config.js/mjs) shows config
    - `npm run build` completes without errors
  </verify>
  <done>
    - Body size limit increased for audio buffer handling
    - Build passes with new configuration
  </done>
</task>

</tasks>

<verification>
1. Full type check passes: `npx tsc --noEmit`
2. Build succeeds: `npm run build`
3. Manual test flow:
   - Find a YouTube video without captions (or temporarily break fetchTranscript to simulate)
   - Submit URL and observe "Retrieving transcript (may take up to 2 minutes...)" status
   - Verify transcript appears with "Source: Audio transcription" label
   - If video has multiple speakers, verify speaker labels appear
4. Verify YouTube-captioned videos still work (should show "Source: YouTube captions")
</verification>

<success_criteria>
- Videos without YouTube captions automatically transcribed via AssemblyAI
- User sees informative status that acknowledges potential wait time
- Transcript source clearly indicated in UI
- Speaker labels appear when multiple speakers detected
- Long transcriptions complete without timeout (on Vercel Pro)
- Existing YouTube caption flow unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/02-robustness/02-02-SUMMARY.md`
</output>
