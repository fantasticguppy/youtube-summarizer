---
phase: 02-robustness
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/actions/process-video.ts
  - src/components/processing-status.tsx
  - src/components/transcript-view.tsx
  - src/app/page.tsx
  - next.config.ts
autonomous: true

must_haves:
  truths:
    - "When YouTube captions unavailable, app automatically transcribes via AssemblyAI"
    - "User sees 'Transcribing audio...' status during AssemblyAI fallback"
    - "Transcript view shows source indicator (YouTube captions vs Audio transcription)"
    - "Transcript shows speaker labels when AssemblyAI detects multiple speakers"
    - "Long transcriptions complete without timeout errors"
  artifacts:
    - path: "src/actions/process-video.ts"
      provides: "Fallback transcription logic"
      contains: "transcribeAudio"
    - path: "src/components/transcript-view.tsx"
      provides: "Source indicator and speaker label display"
      contains: "TranscriptSource"
    - path: "src/components/processing-status.tsx"
      provides: "Transcribing audio status message"
      contains: "transcribing-audio"
  key_links:
    - from: "src/actions/process-video.ts"
      to: "src/lib/assemblyai/transcribe.ts"
      via: "imports transcribeAudio for fallback"
      pattern: "import.*transcribeAudio"
    - from: "src/app/page.tsx"
      to: "src/components/transcript-view.tsx"
      via: "passes transcript source prop"
      pattern: "transcriptSource"
---

<objective>
Integrate AssemblyAI fallback into video processing pipeline and update UI to show transcript source and speaker labels.

Purpose: Complete the robustness feature by wiring backend to frontend and ensuring users understand when audio transcription is used.
Output: Working end-to-end flow where videos without YouTube captions are automatically transcribed via AssemblyAI with proper UI feedback.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-robustness/02-RESEARCH.md
@.planning/phases/02-robustness/02-01-SUMMARY.md

# Files to modify
@src/actions/process-video.ts
@src/components/processing-status.tsx
@src/components/transcript-view.tsx
@src/app/page.tsx
@src/types/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update process-video.ts with AssemblyAI fallback</name>
  <files>
    - src/actions/process-video.ts
  </files>
  <action>
Update `src/actions/process-video.ts` to:
1. Try YouTube captions first (existing behavior)
2. Fall back to AssemblyAI transcription on failure
3. Return transcript source information to frontend

```typescript
'use server';

import { extractVideoId, fetchVideoMetadata, fetchTranscript, formatTranscriptIntoParagraphs } from '@/lib/youtube';
import { transcribeAudio } from '@/lib/assemblyai/transcribe';
import { VideoMetadata, TranscriptSegment, TranscriptSource, SpeakerUtterance } from '@/types';

export interface ProcessVideoResult {
  success: true;
  videoId: string;
  metadata: VideoMetadata;
  transcript: string;
  rawSegments: TranscriptSegment[];
  transcriptSource: TranscriptSource;           // NEW
  utterances?: SpeakerUtterance[];              // NEW
  hasSpeakers: boolean;                         // NEW
}

export interface ProcessVideoError {
  success: false;
  error: string;
  metadata?: VideoMetadata;
}

export async function processVideo(url: string): Promise<ProcessVideoResult | ProcessVideoError> {
  // Step 1: Extract and validate video ID
  const videoId = extractVideoId(url);
  if (!videoId) {
    return { success: false, error: 'Invalid YouTube URL. Please check the URL and try again.' };
  }

  // Step 2: Fetch metadata
  let metadata: VideoMetadata;
  try {
    metadata = await fetchVideoMetadata(videoId);
  } catch (error) {
    return {
      success: false,
      error: 'Could not find video. Please check the URL and try again.',
    };
  }

  // Step 3: Try YouTube captions first
  let rawSegments: TranscriptSegment[];
  let transcript: string;
  let transcriptSource: TranscriptSource = 'youtube';
  let utterances: SpeakerUtterance[] | undefined;
  let hasSpeakers = false;

  try {
    rawSegments = await fetchTranscript(videoId);
    transcript = formatTranscriptIntoParagraphs(rawSegments);
  } catch (youtubeError) {
    // Step 4: Fall back to AssemblyAI
    try {
      const result = await transcribeAudio(videoId);
      transcript = result.text;
      rawSegments = result.segments;
      transcriptSource = 'assemblyai';
      utterances = result.utterances;
      hasSpeakers = result.hasSpeakers;
    } catch (assemblyError) {
      // Both methods failed - return original YouTube error for clarity
      return {
        success: false,
        error: youtubeError instanceof Error
          ? youtubeError.message
          : 'Failed to fetch transcript. This video may not have captions and audio transcription failed.',
        metadata,
      };
    }
  }

  return {
    success: true,
    videoId,
    metadata,
    transcript,
    rawSegments,
    transcriptSource,
    utterances,
    hasSpeakers,
  };
}
```

Key implementation notes:
- Preserve original YouTube error message when both methods fail
- Track source for UI display
- Pass through utterances and hasSpeakers for speaker label display
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep "transcribeAudio" src/actions/process-video.ts` shows import
    - `grep "transcriptSource" src/actions/process-video.ts` shows return field
  </verify>
  <done>
    - YouTube captions tried first (preserves free/fast path)
    - AssemblyAI fallback activates on YouTube failure
    - Result includes transcriptSource, utterances, hasSpeakers
  </done>
</task>

<task type="auto">
  <name>Task 2: Update UI components for transcript source and speakers</name>
  <files>
    - src/components/processing-status.tsx
    - src/components/transcript-view.tsx
    - src/app/page.tsx
  </files>
  <action>
1. Update `src/components/processing-status.tsx` to handle new status:
```typescript
'use client';

import { Loader2 } from 'lucide-react';
import { ProcessingStatus as Status } from '@/types';

interface ProcessingStatusProps {
  status: Status;
}

const statusMessages: Record<Status, string> = {
  idle: '',
  'fetching-metadata': 'Fetching video information...',
  'fetching-transcript': 'Retrieving transcript...',
  'transcribing-audio': 'Transcribing audio (this may take a minute)...',  // NEW
  summarizing: 'Generating summary...',
  complete: 'Done!',
  error: '',
};

export function ProcessingStatus({ status }: ProcessingStatusProps) {
  if (status === 'idle' || status === 'error') {
    return null;
  }

  const isLoading = status !== 'complete';
  const message = statusMessages[status];

  return (
    <div className="flex items-center gap-2 text-sm text-muted-foreground">
      {isLoading && (
        <Loader2 className="h-4 w-4 animate-spin" />
      )}
      <span>{message}</span>
    </div>
  );
}
```

2. Update `src/components/transcript-view.tsx` to show source and speaker labels:
```typescript
'use client';

import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { CopyButton } from './copy-button';
import { TranscriptSource } from '@/types';

interface TranscriptViewProps {
  transcript: string;
  source: TranscriptSource;
  hasSpeakers: boolean;
}

export function TranscriptView({ transcript, source, hasSpeakers }: TranscriptViewProps) {
  const sourceLabel = source === 'youtube'
    ? 'YouTube captions'
    : hasSpeakers
      ? 'Audio transcription (with speakers)'
      : 'Audio transcription';

  return (
    <Card>
      <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
        <div className="space-y-1">
          <CardTitle className="text-lg font-medium">Full Transcript</CardTitle>
          <p className="text-xs text-muted-foreground">
            Source: {sourceLabel}
          </p>
        </div>
        <CopyButton text={transcript} />
      </CardHeader>
      <CardContent>
        <div className="prose prose-sm max-w-none dark:prose-invert">
          <div
            className="whitespace-pre-wrap text-sm leading-relaxed"
            dangerouslySetInnerHTML={{
              __html: formatTranscriptHtml(transcript, hasSpeakers)
            }}
          />
        </div>
      </CardContent>
    </Card>
  );
}

/**
 * Convert markdown speaker labels to HTML for display.
 * Only processes **Speaker X:** patterns when hasSpeakers is true.
 */
function formatTranscriptHtml(text: string, hasSpeakers: boolean): string {
  if (!hasSpeakers) {
    // No speakers - just escape HTML and preserve line breaks
    return escapeHtml(text);
  }

  // Convert **Speaker X:** to styled spans
  return escapeHtml(text).replace(
    /\*\*Speaker ([A-Z]):\*\*/g,
    '<strong class="text-primary">Speaker $1:</strong>'
  );
}

function escapeHtml(text: string): string {
  return text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&#039;');
}
```

3. Update `src/app/page.tsx` to pass new props:
- Add state for `transcriptSource` and `hasSpeakers`
- Pass these to TranscriptView via ResultsTabs

Find the state declarations and add:
```typescript
const [transcriptSource, setTranscriptSource] = useState<TranscriptSource>('youtube');
const [hasSpeakers, setHasSpeakers] = useState(false);
```

In handleSubmit, after successful processVideo:
```typescript
setTranscriptSource(result.transcriptSource);
setHasSpeakers(result.hasSpeakers);
```

Update the ResultsTabs usage to pass these props (will require updating ResultsTabs and TranscriptView interface).

Also update ResultsTabs component (`src/components/results-tabs.tsx`) to accept and pass through these props:
```typescript
interface ResultsTabsProps {
  summary: string;
  transcript: string;
  transcriptSource: TranscriptSource;
  hasSpeakers: boolean;
  isSummarizing: boolean;
}
```

Import TranscriptSource type in page.tsx:
```typescript
import { VideoMetadata, ProcessingStatus as Status, TranscriptSource } from '@/types';
```
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep "transcribing-audio" src/components/processing-status.tsx` shows new status
    - `grep "TranscriptSource" src/components/transcript-view.tsx` shows import
    - `grep "hasSpeakers" src/app/page.tsx` shows state variable
  </verify>
  <done>
    - Processing status shows "Transcribing audio..." during AssemblyAI fallback
    - Transcript view shows source (YouTube captions vs Audio transcription)
    - Speaker labels rendered with styling when hasSpeakers is true
    - All props flow from page.tsx through ResultsTabs to TranscriptView
  </done>
</task>

<task type="auto">
  <name>Task 3: Configure timeout for long-running transcriptions</name>
  <files>
    - next.config.ts
    - src/actions/process-video.ts
  </files>
  <action>
AssemblyAI transcription can take 30-130+ seconds. Configure Next.js to handle this.

1. Check if `next.config.ts` exists, if not create it. If `next.config.js` exists, use that instead.

2. For Vercel deployment, the server action timeout must be configured. Add experimental serverActions config:

```typescript
// next.config.ts (or update existing)
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  experimental: {
    serverActions: {
      bodySizeLimit: '10mb',  // Audio buffers can be large
    },
  },
};

export default nextConfig;
```

3. For the server action itself, add the maxDuration export at the file level in `src/actions/process-video.ts`:

Add at the top of the file (after 'use server'):
```typescript
// Allow up to 5 minutes for AssemblyAI transcription
// This is required for Vercel deployment
export const maxDuration = 300;
```

Note: maxDuration is a Vercel-specific export that configures function timeout.
On Vercel Hobby plan, max is 60 seconds. Pro plan allows up to 300 seconds.
For development, this has no effect (no timeout).
  </action>
  <verify>
    - `grep "maxDuration" src/actions/process-video.ts` shows export
    - `grep "serverActions" next.config.ts` shows config (or next.config.js/mjs)
    - `npm run build` completes without errors
  </verify>
  <done>
    - Server action configured for 5-minute timeout (Vercel Pro) or 60s (Hobby)
    - Body size limit increased for audio buffer handling
    - Build passes with new configuration
  </done>
</task>

</tasks>

<verification>
1. Full type check passes: `npx tsc --noEmit`
2. Build succeeds: `npm run build`
3. Manual test flow:
   - Find a YouTube video without captions (or temporarily break fetchTranscript to simulate)
   - Submit URL and observe "Transcribing audio..." status
   - Verify transcript appears with "Source: Audio transcription" label
   - If video has multiple speakers, verify speaker labels appear
4. Verify YouTube-captioned videos still work (should show "Source: YouTube captions")
</verification>

<success_criteria>
- Videos without YouTube captions automatically transcribed via AssemblyAI
- User sees clear status during audio transcription
- Transcript source clearly indicated in UI
- Speaker labels appear when multiple speakers detected
- Long transcriptions complete without timeout (on Vercel Pro)
- Existing YouTube caption flow unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/02-robustness/02-02-SUMMARY.md`
</output>
